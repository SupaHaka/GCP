#Add a Dataframe to Big Query

import datalab.bigquery as bq
df = <dataframe> # a pandas dataframe

dataset = bq.Dataset('<dataset>')  # define dataset
table = bq.Table('<dataset>' + '.' + '<table_name>') # define table path in BQ
table_schema = bq.Schema.from_data(df) # define schema of dataframe
table.create(schema = table_schema, overwrite = True)
table.insert(df)


#Alternative way of adding pandas dataframe to BQ table (table must exist):

df.to_gbq('<dataset.table>', '<project_id>', if_exists = <'replace', 'fail', 'append'>)

***Adjust the above so we are saving the dataframe to GCS and then loading csv from GCS to BQ***

# We can connect BigQuery to Pandas Dataframe

import pandas a pd
SQL = <SQL Code> # Must include full path to table
df = pd.read_gbq(SQL, project_id = '<>', dialect='[standard,legacy]') 
