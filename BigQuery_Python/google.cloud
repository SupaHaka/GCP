#Create a backup of an existing table in BQ:

from google.cloud import bigquery

def create_back_up(dataset, source_table, destination_table):
    """This function will create a backup of a table.
    Takes 3 arguments: dataset, source table, destination table respectively.
    """
    client = bigquery.Client()
    source_dataset = bigquery.DatasetReference(
        '<project_ID>', dataset) # Project ID might need changing
    source_table_ref = source_dataset.table(source_table)

    dest_dataset = bigquery.Dataset(client.dataset(dataset)) 
    dest_table_ref = dest_dataset.table(destination_table)

    job_config = bigquery.CopyJobConfig()
    job = client.copy_table(
        source_table_ref, dest_table_ref, job_config=job_config)  # API request
    job.result()  # Waits for job to complete.

    assert job.state == 'DONE'
    dest_table = client.get_table(dest_table_ref)  # API request
    return 'A backup of {} has been created'.format(source_table)
create_back_up('<dataset>', '<table to copy>', '<name of copied table>')


#Create a new table in BQ from a SQL query

from google.cloud import bigquery

def Run_SQL_String(SQL, dataset_id, BQ_table, write_type):
 
  query_conf = bigquery.QueryJobConfig()
    # set destination table
  table_ref = client.dataset(dataset_id=dataset_id).table(BQ_table)
    # set table preferences
  query_conf.create_disposition = 'CREATE_IF_NEEDED'
  query_conf.write_disposition = write_type # two possible write_types WRITE_TRUNCATE and WRITE_APPEND
  query_conf.destination = table_ref

    # run query
  run_query = client.query(SQL, job_config=query_conf)

  run_query.result()


#Save dataframe to google cloud storage

from google.cloud import storage

storage_client = storage.Client(project='<>') # create the storage client
bucket = storage_client.get_bucket('<top_level_bucket>') # Identify the top level bucket
destination_blob = bucket.blob('<full_path_plus_file_name>') # create the key to above bucket

file = df_final.to_csv(index=False) # save the dataframe to a variable
destination_blob.upload_from_string(file) # write the dataframe variable to bucket as a string
