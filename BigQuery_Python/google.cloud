#Create a backup of an existing table in BQ:

from google.cloud import bigquery

def create_back_up(dataset, source_table, destination_table):
    """This function will create a backup of a table.
    Takes 3 arguments: dataset, source table, destination table respectively.
    """
    client = bigquery.Client()
    source_dataset = bigquery.DatasetReference(
        '<project_ID>', dataset) # Project ID might need changing
    source_table_ref = source_dataset.table(source_table)

    dest_dataset = bigquery.Dataset(client.dataset(dataset)) 
    dest_table_ref = dest_dataset.table(destination_table)

    job_config = bigquery.CopyJobConfig()
    job = client.copy_table(
        source_table_ref, dest_table_ref, job_config=job_config)  # API request
    job.result()  # Waits for job to complete.

    assert job.state == 'DONE'
    dest_table = client.get_table(dest_table_ref)  # API request
    return 'A backup of {} has been created'.format(source_table)
create_back_up('<dataset>', '<table to copy>', '<name of copied table>')


#Create a new table in BQ from a SQL query

from google.cloud import bigquery

client = bigquery.Client()
job_config = bigquery.QueryJobConfig()

# Set the destination table. Here, dataset_id is a string, such as:
# dataset_id = 'your_dataset_id'
table_ref = client.dataset('<dataset_name>').table('<table_name>')
job_config.destination = table_ref

# The write_disposition specifies the behavior when writing query results
# to a table that already exists. With WRITE_TRUNCATE, any existing rows
# in the table are overwritten by the query results.
job_config.write_disposition = 'WRITE_APPEND' # Different options [WRITE_TRUNCATE, WRITE_APPEND]

SQL = '<SQL_statement>'
# Start the query, passing in the extra configuration.
query_job = client.query(SQL, job_config=job_config)

#Save dataframe to google cloud storage

from google.cloud import storage

storage_client = storage.Client(project='<>') # create the storage client
bucket = storage_client.get_bucket('<top_level_bucket>') # Identify the top level bucket
destination_blob = bucket.blob('<full_path_plus_file_name>') # create the key to above bucket

file = df_final.to_csv(index=False) # save the dataframe to a variable
destination_blob.upload_from_string(file) # write the dataframe variable to bucket as a string
